"""
๐ ุนููู ุงุชุตุงู ุจู Qwen2.5-7B ุนุจุฑ API ุฎุงุฑุฌู
ูุง ูุญูู ุงููููุฐุฌ ูุญููุงู (ูุจูู ุชุญุช 50MB ูู Vercel)
"""

import os
import requests
import json
from typing import Dict, List, Optional
import time

class QwenClient:
    """ุนููู ููุงุชุตุงู ุจู Qwen2.5-7B ุนุจุฑ Hugging Face ุฃู ุฎุฏูุฉ ุฎุงุฑุฌูุฉ"""
    
    def __init__(self):
        # ุงุณุชุฎุฏุงู Hugging Face Inference API (ูุฌุงูู ูุญุฏูุฏ)
        self.api_url = "https://api-inference.huggingface.co/models"
        self.model_name = "Qwen/Qwen2.5-7B-Instruct"
        
        # ุฃู ุงุณุชุฎุฏุงู ุฎุฏูุฉ Ollama ุฅุฐุง ูุงู ุนูุฏู ุณูุฑูุฑ
        self.ollama_url = os.environ.get("OLLAMA_URL", "")
        
        # ููุชุงุญ Hugging Face (ุณุฌูู ูุงุญุตู ุนูู token ูุฌุงูู)
        self.hf_token = os.environ.get("HF_TOKEN", "")
        
        # ูุณุฎุฉ ุงุญุชูุงุทูุฉ ูุญููุฉ ุจุณูุทุฉ
        self.use_backup = False
        
    def query_huggingface(self, prompt: str, max_tokens: int = 300) -> str:
        """ุงุณุชุฎุฏุงู Hugging Face Inference API"""
        
        if not self.hf_token:
            print("โ๏ธ HF_TOKEN ุบูุฑ ูุถุจูุทุ ุงุณุชุฎุฏุงู ุงููุณุฎุฉ ุงูุงุญุชูุงุทูุฉ")
            return self._backup_response(prompt)
        
        headers = {
            "Authorization": f"Bearer {self.hf_token}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": max_tokens,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "return_full_text": False
            }
        }
        
        try:
            response = requests.post(
                f"{self.api_url}/{self.model_name}",
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list):
                    return result[0].get('generated_text', '').strip()
                return result.get('generated_text', '').strip()
            else:
                print(f"โ๏ธ ุฎุทุฃ HuggingFace API: {response.status_code}")
                return self._backup_response(prompt)
                
        except Exception as e:
            print(f"โ๏ธ ูุดู ุงูุงุชุตุงู ุจู HuggingFace: {e}")
            return self._backup_response(prompt)
    
    def query_ollama(self, prompt: str) -> str:
        """ุงุณุชุฎุฏุงู Ollama ุฅุฐุง ูุงู ุนูุฏู ุณูุฑูุฑ"""
        
        if not self.ollama_url:
            return self.query_huggingface(prompt)
        
        payload = {
            "model": "qwen2.5:7b",
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_predict": 300
            }
        }
        
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '').strip()
            return self.query_huggingface(prompt)
            
        except:
            return self.query_huggingface(prompt)
    
    def generate(self, context: str, question: str) -> str:
        """ุชูููุฏ ุฑุฏ ุฐูู ุจุงุณุชุฎุฏุงู Qwen"""
        
        # ุจูุงุก prompt ูุญุณู ูู Qwen2.5-Instruct
        prompt = self._build_prompt(context, question)
        
        # ุงููุญุงููุฉ ูุน Hugging Face ุฃููุงู
        response = self.query_huggingface(prompt)
        
        # ุฅุฐุง ูุดูุ ุฌุฑุจ Ollama
        if not response or len(response) < 10:
            response = self.query_ollama(prompt)
        
        # ุฅุฐุง ูุดู ูู ุดูุกุ ุงุณุชุฎุฏู ุงููุณุฎุฉ ุงูุงุญุชูุงุทูุฉ
        if not response or len(response) < 10:
            response = self._backup_response(question)
        
        return response
    
    def _build_prompt(self, context: str, question: str) -> str:
        """ุจูุงุก prompt ูุญุณู ูู Qwen2.5-Instruct"""
        
        system_prompt = """ุฃูุช ูุณุงุนุฏ ุนูุงุฏุฉ ุฃุณูุงู ุฐูู ููุชุฎุตุต. 
ูููุชู ูุณุงุนุฏุฉ ุงููุฑุถู ุจุงูุฅุฌุงุจุฉ ุนูู ุงุณุชูุณุงุฑุงุชูู ุจุทุฑููุฉ ููููุฉ ููููุฏุฉ.

ุชูุฌููุงุช ูููุฉ:
1. ุฃุฌุจ ุจูุบุฉ ุนุฑุจูุฉ ูุตูุญุฉ ููุงุถุญุฉ
2. ุงุณุชุฎุฏู ุงููุนูููุงุช ุงูููุฏูุฉ ููุท - ูุง ุชุฎุชูู ูุนูููุงุช
3. ูู ุฏูููุงู ูู ุฐูุฑ ุงูุชูุงุตูู
4. ุฅุฐุง ูุงู ุงูุณุคุงู ูุญุชุงุฌ ูุนูููุงุช ุบูุฑ ูุชููุฑุฉุ ูู ุจุตุฑุงุญุฉ "ูุง ุฃุนุฑู" ููุตุญ ุจุงูุงุชุตุงู ุจุงูุนูุงุฏุฉ
5. ูู ููููุงู ููุชุนุงุทูุงู ูุน ุงููุฑุถู

ุงููุนูููุงุช ุงููุชุงุญุฉ:"""
        
        prompt = f"""{system_prompt}

{context}

ุณุคุงู ุงููุฑูุถ: {question}

ุฃุฌุจ ุจุทุฑููุฉ ูููุฏุฉ ูููููุฉุ ูุน ุงูุชุฑููุฒ ุนูู ุชูุฏูู ุงููุนูููุงุช ุงูุฃูุซุฑ ุฃูููุฉ ูููุฑูุถ.
ุชุฐูุฑ ุฃู ุชููู ุฏูููุงู ูู ุฐูุฑ ุงูุฃุฑูุงู ูุงูุนูุงููู ุฅุฐุง ูุงูุช ูุชููุฑุฉ.

ุงูุฅุฌุงุจุฉ:"""
        
        return prompt
    
    def _backup_response(self, question: str) -> str:
        """ุฑุฏ ุงุญุชูุงุทู ุฅุฐุง ูุดู ุงูุงุชุตุงู"""
        
        question_lower = question.lower()
        
        if any(word in question_lower for word in ['ูุฑุญุจุง', 'ุงููุง', 'ุงูุณูุงู']):
            return "ูุฑุญุจุงู! ๐ ุฃูุง ูุณุงุนุฏ ุนูุงุฏุฉ ุงูุฃุณูุงู ุงูุฐูู. ููู ูููููู ูุณุงุนุฏุชู ุงููููุ"
        
        elif any(word in question_lower for word in ['ุญุฌุฒ', 'ููุนุฏ']):
            return """๐ ูุญุฌุฒ ููุนุฏ ูู ุนูุงุฏุฉ ุงูุฃุณูุงู:
โข ุงุชุตู ุจูุง ุนูู: 0112345678
โข ุฃู ุนุจุฑ ุงููุงุชุณุงุจ: 0551234567
โข ูู ุงูุฃุญุฏ ุฅูู ุงูุฎููุณ: 8 ุตุจุงุญุงู - 8 ูุณุงุกู
โข ุณูุงุณุฉ ุงูุฅูุบุงุก: ูุฌุงูู ูุจู 24 ุณุงุนุฉ"""
        
        elif any(word in question_lower for word in ['ุณุนุฑ', 'ุชูููุฉ', 'ูู']):
            return """๐ฐ ุงูุฃุณุนุงุฑ ุงูุชูุฑูุจูุฉ:
โข ุงููุดู ูุงูุชุดุฎูุต: 100 ุฑูุงู
โข ุชูุธูู ุงูุฃุณูุงู: 150 ุฑูุงู
โข ุญุดู ุงูุฃุณูุงู: 200-350 ุฑูุงู
โข ุนูุงุฌ ุงูุนุตุจ: 500-800 ุฑูุงู
โข ุชูููู ุงูุฃุณูุงู: ูุจุฏุฃ ูู 5000 ุฑูุงู

ููุงุญุธุฉ: ุงูุฃุณุนุงุฑ ูุฏ ุชุฎุชูู ุญุณุจ ุงูุญุงูุฉ."""
        
        elif any(word in question_lower for word in ['ุนููุงู', 'ุงูู', 'ููุงู']):
            return """๐ ุนูุงุฏุฉ ุงูุฃุณูุงู:
โข ุงูุนููุงู: ุดุงุฑุน ุงูููู ููุฏุ ุญู ุงูุนููุงุ ุงูุฑูุงุถ
โข ุงููุงุชู: 0112345678
โข ุงููุงุชุณุงุจ: 0551234567
โข ุงูุจุฑูุฏ: info@dental-smile.com"""
        
        elif any(word in question_lower for word in ['ููุช', 'ุฏูุงู', 'ูุชู']):
            return """๐ ุฃููุงุช ุงูุนูู:
โข ุงูุฃุญุฏ ุฅูู ุงูุฎููุณ: 8:00 ุตุจุงุญุงู - 8:00 ูุณุงุกู
โข ุงูุฌูุนุฉ ูุงูุณุจุช: ุฅุฌุงุฒุฉ
โข ๐ ุทูุงุฑุฆ 24 ุณุงุนุฉ: 0551234567"""
        
        elif any(word in question_lower for word in ['ุทุงุฑุฆ', 'ุนุงุฌู', 'ุฃูู']):
            return """๐จ ููุญุงูุงุช ุงูุทุงุฑุฆุฉ:
โข ุงุชุตู ููุฑุงู ุนูู: 0551234567
โข ููููู ุงูุญุถูุฑ ูุจุงุดุฑุฉ ููุนูุงุฏุฉ
โข ูุฑูู ุงูุทูุงุฑุฆ ูุชุงุญ 24 ุณุงุนุฉ
โข ูุง ุชุญุชุงุฌ ููุนุฏ ูุณุจู ููุญุงูุงุช ุงูุญุฑุฌุฉ"""
        
        else:
            return """ูุฑุญุจุงู! ูููููู ูุณุงุนุฏุชู ูู:
โข ุญุฌุฒ ุงูููุงุนูุฏ ูุงูุฒูุงุฑุงุช
โข ูุนูููุงุช ุงูุฃุณุนุงุฑ ูุงูุฎุฏูุงุช
โข ุงูุนููุงู ูุทุฑู ุงูุชูุงุตู
โข ุฃููุงุช ุงูุนูู ูุงูุทูุงุฑุฆ
โข ุงุณุชูุณุงุฑุงุช ุทุจูุฉ ุนุงูุฉ

ูุงุฐุง ุชุฑูุฏ ุฃู ุชุนุฑูุ ๐"""
